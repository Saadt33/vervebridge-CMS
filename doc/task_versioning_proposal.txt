PROPOSAL FOR HANDLING TASK VERSIONS IN A CONTEST
================================================



*** Rationale

In this draft we examine a possible implementation of task versioning.
First of all, also to explain the term "versioning", we explicit the
results we intend to obtain.

1. Ability to have more than one version of a task, differing by
   test-cases (usually) but also managers, time/memory limit... One of
   these versions is the "live" version and behaves as the only one in
   the current implementation. The others are "hidden" versions.

2. Ability to evaluate the same set of submissions on a hidden version
   of the task (automatically?).

3. Ability to switch seamlessly from one version to another.

4. Ability to clone a version to a new one.



*** Use cases

* Roberto is a CMS admin. He is not sure of the Pascal implementation
  of the manager for the communication task of an IOI contest. He then
  loads two version of the task, one for each of the two
  tentatives. When the contest starts, he uses as the live version the
  one he thinks will succeed, but it evaluates the submissions also
  with the other version. In the middle of the contest, a submission
  arrives that highlight a bug in the first version: no worries!
  Roberto just switch to the second version and inform the contestants
  of a re-evaluation, with zero score-downtime.

* Stefano is a CMS admin. He always messes up with input
  generations. In the middle of a contest, he realizes that one of the
  input is invalid. He then clones the task to a new version, changes
  the invalid input to a correct one, launches the re-evaluation on
  the cloned version (that is still hidden). When the re-evaluation
  finishes, he switches to the cloned version and announces a new
  evaluation, with zero score-downtime.



*** Implementation


** Tasks & datasets

We change the tasks table, leaving only the information about the task
that do not change amongst versions (for sanity or because it is
something the users can see):

* id, name, title, num, statement;

* contest-id;

* token_*;

* attachments, submission_format_elements.

Tasks also stores a "live_dataset_id" field indicating which dataset
is alive. We add a datasets table, holding all the task information
not said before, and also an "id" and a "description" field (used to
offer a human-readable description of why that version is there and in
what it differs from other versions. The pair (task_id, id) is the
primary key.


** Other changes

Looking at the big picture, there will be some other things to change,
in particular because we want to fulfill aim #2. This implies that we
need to be able to store evaluations and scores on a per version
basis, and not per task.

* evaluations are easily corrected adding a dataset_id field;

* (public_)scores(_details) need to be detached from the submission
  object and handled in a table by themselves holding the data and the
  ids of the submission, the task and the dataset;

* we need to detach also compilation/evaluation_outcome/text/tries
  etc. from the submission. Sadly some of these are user-visible
  fields, but different versions of a task should not introduce
  noticeable differences in them.

Also, to handle evaluation of a hidden version, we need ES to support
more levels of priority in order to give hidden evaluations the lowest
possible.


** Interface

We also will need some big interface effort to implement the new tasks
and datasets and be able to do the switching and the reevaluation. Our
admin interfaces is already quite cluttered, so my proposal is that
versions-related information is displayed only in the tasks'
page. Contestants should not see any change in their interface.
